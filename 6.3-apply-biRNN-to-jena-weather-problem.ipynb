{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 로드하고 제너레이터를 만드는 아래 코드는 [6.3-advanced-usage-of-recurrent-neural-networks](6.3-advanced-usage-of-recurrent-neural-networks.ipynb) 노트북과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
      "420551\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = './datasets/jena_climate/'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step, \n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "# This is how many steps to draw from `val_gen`\n",
    "# in order to see the whole validation set:\n",
    "val_steps = (300000 - 200001 - lookback) // batch_size\n",
    "\n",
    "# This is how many steps to draw from `test_gen`\n",
    "# in order to see the whole test set:\n",
    "test_steps = (len(float_data) - 300001 - lookback) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the same approach on the weather prediction task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.2915 - val_loss: 0.2734\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 168s 336ms/step - loss: 0.2739 - val_loss: 0.2662\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 166s 331ms/step - loss: 0.2680 - val_loss: 0.2652\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 166s 331ms/step - loss: 0.2617 - val_loss: 0.2677\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.2528 - val_loss: 0.2633\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.2454 - val_loss: 0.2684\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 166s 331ms/step - loss: 0.2420 - val_loss: 0.2702\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 166s 331ms/step - loss: 0.2326 - val_loss: 0.2844\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 166s 331ms/step - loss: 0.2287 - val_loss: 0.2787\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.2209 - val_loss: 0.2866\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.2161 - val_loss: 0.2921\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.2091 - val_loss: 0.2974\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.2055 - val_loss: 0.2974\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1994 - val_loss: 0.3028\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1942 - val_loss: 0.3071\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1909 - val_loss: 0.3137\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1851 - val_loss: 0.3125\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1807 - val_loss: 0.3156\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1779 - val_loss: 0.3162\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1746 - val_loss: 0.3174\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1710 - val_loss: 0.3175\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1680 - val_loss: 0.3241\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 166s 331ms/step - loss: 0.1658 - val_loss: 0.3184\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1630 - val_loss: 0.3303\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1610 - val_loss: 0.3290\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1574 - val_loss: 0.3296\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1558 - val_loss: 0.3317\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1530 - val_loss: 0.3265\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1516 - val_loss: 0.3313\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1488 - val_loss: 0.3323\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1481 - val_loss: 0.3297\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1462 - val_loss: 0.3296\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1453 - val_loss: 0.3357\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1433 - val_loss: 0.3345\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1416 - val_loss: 0.3319\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1394 - val_loss: 0.3304\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1394 - val_loss: 0.3355\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1377 - val_loss: 0.3354\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 165s 331ms/step - loss: 0.1386 - val_loss: 0.3347\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 165s 330ms/step - loss: 0.1356 - val_loss: 0.3351\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(\n",
    "    layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs about as well as the regular GRU layer. It's easy to understand why: all of the predictive capacity must be coming from the \n",
    "chronological half of the network, since the anti-chronological half is known to be severely underperforming on this task (again, because \n",
    "the recent past matters much more than the distant past in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Going even further\n",
    "\n",
    "At this stage, there are still many other things you could try in order to improve performance on our weather forecasting problem:\n",
    "\n",
    "* Adjust the number of units in each recurrent layer in the stacked setup. Our current choices are largely arbitrary and thus likely \n",
    "suboptimal.\n",
    "* Adjust the learning rate used by our `RMSprop` optimizer.\n",
    "* Try using `LSTM` layers instead of `GRU` layers.\n",
    "* Try using a bigger densely-connected regressor on top of the recurrent layers, i.e. a bigger `Dense` layer or even a stack of `Dense` \n",
    "layers.\n",
    "* Don't forget to eventually run the best performing models (in terms of validation MAE) on the test set! Least you start developing \n",
    "architectures that are overfitting to the validation set.   \n",
    "\n",
    "As usual: deep learning is more an art than a science, and while we can provide guidelines as to what is likely to work or not work on a \n",
    "given problem, ultimately every problem is unique and you will have to try and evaluate different strategies empirically. There is \n",
    "currently no theory that will tell you in advance precisely what you should do to optimally solve a problem. You must try and iterate.\n",
    "\n",
    "\n",
    "## Wrapping up\n",
    "\n",
    "Here's what you should take away from this section:\n",
    "\n",
    "* As you first learned in Chapter 4, when approaching a new problem, \n",
    "it is good to first establish common sense baselines for your metric of choice. If you don't have a \n",
    "baseline to beat, you can't tell if you are making any real progress.\n",
    "* Try simple models before expensive ones, to justify the additional expense. Sometimes a simple model will turn out to be your best option.\n",
    "* On data where temporal ordering matters, recurrent networks are a great fit and easily outperform models that first flatten the temporal \n",
    "data.\n",
    "* To use dropout with recurrent networks, one should use a time-constant dropout mask and recurrent dropout mask. This is built into Keras \n",
    "recurrent layers, so all you have to do is use the `dropout` and `recurrent_dropout` arguments of recurrent layers.\n",
    "* Stacked RNNs provide more representational power than a single RNN layer. They are also much more expensive, and thus not always worth it. \n",
    "While they offer clear gains on complex problems (e.g. machine translation), they might not always be relevant to smaller, simpler problems.\n",
    "* Bidirectional RNNs, which look at a sequence both ways, are very useful on natural language processing problems. However, they will not \n",
    "be strong performers on sequence data where the recent past is much more informative than the beginning of the sequence.\n",
    "\n",
    "Note there are two important concepts that we will not cover in detail here: recurrent \"attention\", and sequence masking. Both tend to be \n",
    "especially relevant for natural language processing, and are not particularly applicable to our temperature forecasting problem. We will \n",
    "leave them for future study outside of this book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
